{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821029e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active code page: 1252\n",
      "Requirement already satisfied: openai in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\satya\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\satya\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\satya\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\satya\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\satya\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649093a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db159df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"mistralai/mistral-7b-instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c96b4",
   "metadata": {},
   "source": [
    "cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279e9289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "stars",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "17843695-a1db-4ac2-b262-4dfea2a02254",
       "rows": [
        [
         "0",
         "Meh.\n\nPlus for cool decor. Minus for being in a strip mall (but hey it's Phoenix...)\n\nCinnamon Roll was good.  Awesome icing. Toluca Brizza was unique, but nothing spectacular.\n\nit would be a good place for groups, with a vast menu (that i hear is very vegan friendly), but they are better places around town for brunch, both in terms of quality and price points.",
         "3"
        ],
        [
         "1",
         "Very consistent, thin crust pizza, made with fresh ingredients and excellent cheese! The market place is lively and the atmosphere is festive!",
         "4"
        ],
        [
         "2",
         "While we were in Mesa AZ for a holiday we found Alessia's on Yelp and we are so glad we did. The food was delicious and the host was terrific as were the servers. We are not big wine drinkers but they have a very good selection of wines and the host knows his stuff and can make good recommendations. Nicely appointed little restaurant with a great menu.",
         "5"
        ],
        [
         "3",
         "Exactly what a hole in the wall Pho place should be like! Totally reminds me of LA, SF or any authentic Pho place in any multi-cultural city. Congrats Phoenix for not being completely white bred and redneck- must give thanks to center city.\n\nAnyhow, portions are large, ingredients taste fresh and prices are inexpensive. If you're one of the sheltered folk out here that gets freaked out by rude bums or the lack of decor- stay in the shelter of your HOA tract home or go to the mall.",
         "5"
        ],
        [
         "4",
         "I used to work near here and we got take out a lot. \n\nMy favorites: spinach enchiladas, green corn tamale, chicken mini chimis ( they come with a green chile cream cheese dip, OMG). \n\nI loved their tex mex spin, not authentic Mexican, mind you, but tasty. \n\nI can't speak to the service, I rarely went in to pick up, even... I'm one of those crafty people who can get others to do that, lol. \n\nI liked it though.",
         "4"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meh.\\n\\nPlus for cool decor. Minus for being i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Very consistent, thin crust pizza, made with f...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While we were in Mesa AZ for a holiday we foun...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exactly what a hole in the wall Pho place shou...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I used to work near here and we got take out a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  stars\n",
       "0  Meh.\\n\\nPlus for cool decor. Minus for being i...      3\n",
       "1  Very consistent, thin crust pizza, made with f...      4\n",
       "2  While we were in Mesa AZ for a holiday we foun...      5\n",
       "3  Exactly what a hole in the wall Pho place shou...      5\n",
       "4  I used to work near here and we got take out a...      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\satya\\Downloads\\fynd-ai-task1-rating-prediction\\task\\data\\processed\\yelp_processed.csv\"\n",
    ")\n",
    "\n",
    "df = df[[\"review_text\", \"stars\"]]\n",
    "df.dropna(inplace=True)\n",
    "df[\"stars\"] = df[\"stars\"].astype(int)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9bfec",
   "metadata": {},
   "source": [
    "CELL 4 — Prompt Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc69a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(prompt_file, review_text):\n",
    "    with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        template = f.read()\n",
    "    return template.replace(\"{{review_text}}\", review_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c94f420",
   "metadata": {},
   "source": [
    "CELL 5 — Gemini Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b5c3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def safe_parse_json(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    # Remove markdown/code block wrappers if present\n",
    "    text = re.sub(r\"```json|```\", \"\", text).strip()\n",
    "\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "\n",
    "        if (\n",
    "            isinstance(data, dict)\n",
    "            and \"predicted_stars\" in data\n",
    "            and \"explanation\" in data\n",
    "        ):\n",
    "            return {\n",
    "                \"predicted_stars\": int(data[\"predicted_stars\"]),\n",
    "                \"explanation\": data[\"explanation\"],\n",
    "                \"valid_json\": True\n",
    "            }\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # True fallback (used only if parsing fails)\n",
    "    return {\n",
    "        \"predicted_stars\": None,\n",
    "        \"explanation\": None,\n",
    "        \"valid_json\": False\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ac1dc",
   "metadata": {},
   "source": [
    "cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46ed6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_review(review_text, prompt_file):\n",
    "    prompt = load_prompt(prompt_file, review_text)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a strict JSON-only response generator.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    output_text = response.choices[0].message.content\n",
    "    return safe_parse_json(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "059b8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_stars': 3, 'explanation': 'The review is mixed, with positive notes on decor and some menu items, but overall lukewarm satisfaction.'}\n"
     ]
    }
   ],
   "source": [
    "test_review = df[\"review_text\"].iloc[0]\n",
    "\n",
    "result = predict_single_review(\n",
    "    test_review,\n",
    "    \"prompt_v3_structured.txt\"\n",
    ")\n",
    "\n",
    "# FINAL OUTPUT AS PER ASSIGNMENT FORMAT\n",
    "final_output = {\n",
    "    \"predicted_stars\": result[\"predicted_stars\"],\n",
    "    \"explanation\": result[\"explanation\"]\n",
    "}\n",
    "\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b687c23",
   "metadata": {},
   "source": [
    "CELL 7 — Run ALL 3 Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01ddc535",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DIR = r\"C:\\Users\\satya\\Downloads\\fynd-ai-task1-rating-prediction\\task\\prompts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef958d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_prompt(prompt_file, review_text):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "    with open(prompt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        template = f.read()\n",
    "    return template.replace(\"{{review_text}}\", review_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d01e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def safe_parse_json(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    text = re.sub(r\"```json|```\", \"\", text).strip()\n",
    "\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if \"predicted_stars\" in data and \"explanation\" in data:\n",
    "            return {\n",
    "                \"predicted_stars\": int(data[\"predicted_stars\"]),\n",
    "                \"explanation\": data[\"explanation\"],\n",
    "                \"valid_json\": True\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        \"predicted_stars\": None,\n",
    "        \"explanation\": None,\n",
    "        \"valid_json\": False\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f28788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def run_prompt(prompt_file, sample_size=10):\n",
    "    results = []\n",
    "    sample_df = df.sample(sample_size, random_state=42)\n",
    "\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        pred = predict_single_review(row[\"review_text\"], prompt_file)\n",
    "\n",
    "        results.append({\n",
    "            \"actual_stars\": row[\"stars\"],\n",
    "            \"predicted_stars\": pred[\"predicted_stars\"],\n",
    "            \"explanation\": pred[\"explanation\"],\n",
    "            \"valid_json\": pred[\"valid_json\"]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bb6ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.94s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"Zero-Shot\": run_prompt(\"prompt_v1_zero_shot.txt\", sample_size=10),\n",
    "    \"Few-Shot\": run_prompt(\"prompt_v2_few_shot.txt\", sample_size=10),\n",
    "    \"Structured\": run_prompt(\"prompt_v3_structured.txt\", sample_size=10)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20eb5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(df):\n",
    "    total = len(df)\n",
    "    correct = (df[\"actual_stars\"] == df[\"predicted_stars\"]).sum()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    json_validity = df[\"valid_json\"].mean()  # True = 1, False = 0\n",
    "\n",
    "    return accuracy, json_validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "121fab0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Prompt Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "JSON Validity Rate",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "5ab66a13-ba24-4d7c-a7f1-d760ad7c0523",
       "rows": [
        [
         "0",
         "Zero-Shot",
         "0.4",
         "0.4"
        ],
        [
         "1",
         "Few-Shot",
         "0.5",
         "0.5"
        ],
        [
         "2",
         "Structured",
         "0.9",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt Type</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>JSON Validity Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-Shot</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Few-Shot</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Structured</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prompt Type  Accuracy  JSON Validity Rate\n",
       "0   Zero-Shot       0.4                 0.4\n",
       "1    Few-Shot       0.5                 0.5\n",
       "2  Structured       0.9                 1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = []\n",
    "\n",
    "for prompt_name, df_result in results.items():\n",
    "    acc, json_rate = evaluate_results(df_result)\n",
    "    summary.append({\n",
    "        \"Prompt Type\": prompt_name,\n",
    "        \"Accuracy\": round(acc, 3),\n",
    "        \"JSON Validity Rate\": round(json_rate, 3)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(summary)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6af61b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.to_csv(\"task1_prompt_comparison.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36873c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt dir exists: True\n",
      "Files: ['prompt_v1_zero_shot.txt', 'prompt_v2_few_shot.txt', 'prompt_v3_structured.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROMPT_DIR = r\"C:\\Users\\satya\\Downloads\\fynd-ai-task1-rating-prediction\\task\\prompts\"\n",
    "\n",
    "print(\"Prompt dir exists:\", os.path.exists(PROMPT_DIR))\n",
    "print(\"Files:\", os.listdir(PROMPT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7ee7063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:23<00:00,  2.36s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.26s/it]\n",
      "100%|██████████| 10/10 [00:18<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"v1\": run_prompt(\"prompt_v1_zero_shot.txt\", sample_size=10),\n",
    "    \"v2\": run_prompt(\"prompt_v2_few_shot.txt\", sample_size=10),\n",
    "    \"v3\": run_prompt(\"prompt_v3_structured.txt\", sample_size=10)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "728ed42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"task/outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def save_clean_json(df, filename):\n",
    "    clean_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Skip NaN or fallback rows\n",
    "        if (\n",
    "            pd.notna(row[\"predicted_stars\"])\n",
    "            and row[\"explanation\"] != \"Fallback applied.\"\n",
    "        ):\n",
    "            clean_rows.append({\n",
    "                \"predicted_stars\": int(row[\"predicted_stars\"]),\n",
    "                \"explanation\": row[\"explanation\"]\n",
    "            })\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIR, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clean_rows, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(clean_rows)} clean records to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c8543b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 6 clean records to prompt_v1_outputs.json\n",
      "Saved 5 clean records to prompt_v2_outputs.json\n",
      "Saved 10 clean records to prompt_v3_outputs.json\n"
     ]
    }
   ],
   "source": [
    "save_clean_json(results[\"v1\"], \"prompt_v1_outputs.json\")\n",
    "save_clean_json(results[\"v2\"], \"prompt_v2_outputs.json\")\n",
    "save_clean_json(results[\"v3\"], \"prompt_v3_outputs.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a29b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
